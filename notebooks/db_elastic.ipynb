{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuzzPHd6gIUQ",
    "outputId": "a34fedfd-7340-43a9-90d7-4952f665db1e"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "\n",
    "# See docs/elastic.md to start Elastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic DB Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting genai_utils/db_elastic.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile genai_utils/db_elastic.py\n",
    "#!/usr/bin/env python \n",
    "\n",
    "'''\n",
    "RUN as 'python -m genai_utils.db_elastic -p \"\n",
    "'''\n",
    "\n",
    "import os, sys, logging, argparse, glob, hashlib\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_elasticsearch import (\n",
    "    BM25Strategy,\n",
    "    DenseVectorStrategy,\n",
    "    ElasticsearchStore,\n",
    ")\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from mangorest.mango import webapi\n",
    "from genai_utils import pdf_parser\n",
    "from genai_utils import extract_docs\n",
    "\n",
    "logger = logging.getLogger( \"gpt\" )\n",
    "\n",
    "ES_URL, ES_USER, ES_PW  = \"http://localhost:9200\", \"elastic\", \"elastic\"\n",
    "\n",
    "from genai_utils import config\n",
    "\n",
    "ES_CNX= dict(es_url= ES_URL, es_user= ES_USER, es_password=ES_PW)\n",
    "\n",
    "\n",
    "_ES_STARTEGIES = {\n",
    "    \"hnsw\":     DenseVectorStrategy(), \n",
    "    \"bm25\":     BM25Strategy(),\n",
    "    \"hybrid\":   DenseVectorStrategy(hybrid=True, rrf=False),\n",
    "    \"sparse\":   None,\n",
    "    \"exact\":    None,\n",
    "}\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def esDeleteIndex(index=\"test\", url=ES_URL, user=ES_USER, pw= ES_PW, **kwargs):\n",
    "    esclient = Elasticsearch(url, basic_auth = (user, pw))\n",
    "    esclient.info()\n",
    "    try:\n",
    "        esclient.indices.delete(index=index)\n",
    "    except:\n",
    "        pass\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def esCreateIndex(index=\"test\", url=ES_URL, user=ES_USER, pw= ES_PW, **kwargs):\n",
    "    esclient = Elasticsearch(url, basic_auth = (user, pw))\n",
    "    esclient.indices.create(index=index)\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def esCountIndex(index=\"test\", url=ES_URL, user=ES_USER, pw= ES_PW, **kwargs):\n",
    "    doc_count = 0\n",
    "    try:\n",
    "        esclient = Elasticsearch(url, basic_auth = (user, pw))\n",
    "        doc_count = esclient.count(index=index)\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"Total documents in index '{index}': {doc_count}\")\n",
    "    return doc_count\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def getEmbedding(model=\"all-minilm:L6-v2\", base_url = \"http://127.0.0.1:11434/\"):\n",
    "    e = OllamaEmbeddings( model = model, base_url =base_url )\n",
    "    return e\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def getbyID( index=\"test\",id=\"\", url=ES_URL, user=ES_USER, pw= ES_PW, **kwargs):\n",
    "    es_cnx = dict(es_url= ES_URL, es_user=ES_USER, es_password=ES_PW)\n",
    "    doc = ElasticsearchStore.get_by_ids(ids=[id]\n",
    "            **es_cnx,\n",
    "            index_name=index)\n",
    "    return doc\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def add_to_es( docs: list[Document], es_cnx: dict, index: str, embed, strategy= \"hnsw\" ):\n",
    "    strat = _ES_STARTEGIES[strategy]\n",
    "    vectorstore = None\n",
    "    for i in range(0, len(docs), 20000):\n",
    "        docsWithID = docs[i : min(i + 20000, len(docs))]\n",
    "        for d in docsWithID:\n",
    "            h = hashlib.md5(d.page_content.encode())\n",
    "            d.id = h.hexdigest()\n",
    "        \n",
    "        vectorstore = ElasticsearchStore.from_documents(\n",
    "            documents=docsWithID,\n",
    "            embedding=embed,\n",
    "            **es_cnx,\n",
    "            index_name=index,\n",
    "            bulk_kwargs={\n",
    "                \"chunk_size\": 100,\n",
    "            },\n",
    "            strategy=strat,\n",
    "        )\n",
    "    return vectorstore\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def es_retriever( es_cnx: dict, index: str, embed, strategy=\"hnsw\", k= 10 ):\n",
    "    strat = _ES_STARTEGIES[strategy]\n",
    "\n",
    "    v = ElasticsearchStore( **es_cnx, embedding=embed, index_name=index, strategy=strat)\n",
    "    return v.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "def esVectorSearch( retreiver, q, k=10):\n",
    "        ret = retreiver.as_retriever(search_kwargs={\"k\": k}).invoke(q)\n",
    "        \n",
    "        h = {r.page_content:r for r in ret}\n",
    "        if len(h) != len(ret):\n",
    "            ret = [v for v in h.values()]\n",
    "            \n",
    "        return ret\n",
    "\n",
    "@webapi(\"/gpt/esSearchIndex/\")\n",
    "def esSearchIndex(request, index_name, query, model=\"all-minilm:L6-v2\", user=\"\", es_url=\"\", \n",
    "                    es_user=\"\", es_pass=\"\", k=10, rank=1, **kwargs):\n",
    "\n",
    "    #print(f\"\\n{locals()}\\n\")\n",
    "        \n",
    "    if (not es_url):\n",
    "        es = dict(es_url= ES_URL, es_user=ES_USER, es_password=ES_PW)\n",
    "    else:\n",
    "        es = dict(es_url= es_url, es_user=es_user, es_password=es_pass)\n",
    "\n",
    "    #model = \"llama3.2\" #lets force the embedding for now\n",
    "    embed = getEmbedding(model=model) \n",
    "\n",
    "    \n",
    "    #if not os.path.exists(os.path.expanduser(\"~/.cache/RERANKER/\")):\n",
    "    #    print(f\"**** Ranker cache does not exist ****\")\n",
    "    #    return ret\n",
    "    if ( rank):\n",
    "        v = es_retriever(es, index=index_name, embed=embed, k=k*2)\n",
    "        docs = v.invoke(query)\n",
    "        if (len(docs)):\n",
    "            ranked = rerank( query, docs)\n",
    "            docs = [Document(page_content=r['text'], metadata=r['metadata']) for r in ranked[0:k]]\n",
    "    else:\n",
    "        v = es_retriever(es, index=index_name, embed=embed, k=k)\n",
    "        docs = v.invoke(query)\n",
    "\n",
    "    h = {r.page_content: r for r in docs}\n",
    "    if len(h) != len(docs):\n",
    "        docs = [v for v in h.values()]\n",
    "    \n",
    "    ret = []\n",
    "    for d in docs:\n",
    "        ret.append(dict(page_content=d.page_content, metadata=d.metadata))\n",
    "    return ret\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def format(d, show=1):\n",
    "    from IPython.display import HTML\n",
    "    m=d['metadata']\n",
    "    page = m['page'] if \"page\" in m  else \"?\"\n",
    "    html=f'''\n",
    "<h3>Document, {page} : {m['source']} </h3> \n",
    "\n",
    "{d['page_content'].replace(\"\\n\", \"<br>\")}\n",
    "<hr/>\n",
    "'''\n",
    "    if(show):\n",
    "        display(HTML(html))\n",
    "    return html\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "@webapi(\"/gpt/esTextSearch/\")\n",
    "def esTextSearch(query, k=10, index_name=\"test\", url = ES_URL, user=ES_USER, pw= ES_PW):\n",
    "    esclient = Elasticsearch(url, basic_auth = (user, pw))\n",
    "    res = esclient.search(index=index_name,  q=query, size=k)\n",
    "\n",
    "    ret = []\n",
    "    for i,r in enumerate(res['hits']['hits']):\n",
    "        pc = r['_source']['text']\n",
    "        mt = r['_source']['metadata']\n",
    "        ret.append(dict(page_content = pc, metadata=mt))\n",
    "        #print(i, \" ==>\", )\n",
    "    return ret\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def rerank(q, ret):\n",
    "    from flashrank import (Ranker, RerankRequest,)\n",
    "    \n",
    "    ranker = Ranker(\"ms-marco-MiniLM-L-12-v2\", os.path.expanduser(\"~/.cache/RERANKER/\"))\n",
    "    rerankrequest = RerankRequest(\n",
    "        query=q, passages=[{\"text\": d.page_content, \"metadata\": d.metadata} for d in ret]\n",
    "    )\n",
    "    reranked = ranker.rerank(rerankrequest)\n",
    "    return reranked\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# This is standing by itself - should be called by indexFromFolder\n",
    "# can be multi tasked \n",
    "def loadES( model=\"all-minilm:L6-v2\", index=\"\", filename = \"/Users/e346104/Desktop/data/LLM/sample.pdf\",\n",
    "           es_url=ES_URL , es_user=ES_USER, es_password=ES_PW ):\n",
    "    \n",
    "    docs = extract_docs.extractDocs(file=filename)\n",
    "    if (not docs):\n",
    "        return docs\n",
    "    embed= getEmbedding(model)\n",
    "    es = dict(es_url=es_url , es_user=es_user, es_password=es_password)\n",
    "    v = add_to_es(docs, es, index=index, embed=embed)\n",
    "\n",
    "    return docs\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def indexFromFolder(folder=\"\", force=0, index=\"test\", recurse=0, just_show=0,\n",
    "                        url=ES_URL, user=ES_USER, pw= ES_PW, model=\"all-minilm:L6-v2\"):\n",
    "    folder = os.path.expanduser(folder) + \"/**\"\n",
    "    files = [f for f in glob.glob(folder, recursive=recurse) if os.path.isfile(f)]\n",
    "\n",
    "    logger.info(f\"Indexing files from {folder}: found {len(files)} files.\")\n",
    "\n",
    "    iFiles = []\n",
    "    for f in files:\n",
    "        bn = os.path.basename(f)\n",
    "        dn = os.path.dirname(f)\n",
    "        marker = f\"/tmp/gpt/{dn}/.{bn}.{index}.indexed\"\n",
    "\n",
    "        if f.endswith(\".indexed\") or (os.path.exists( marker) and not force):\n",
    "            continue;\n",
    "\n",
    "        try:\n",
    "            if ( not just_show):\n",
    "                pass\n",
    "                logger.info(f\"Indexing '{f}'\")        \n",
    "                loadES(model, index, f, url, user, pw)\n",
    "                os.makedirs(os.path.dirname(marker), exist_ok=True)\n",
    "                open(marker, \"w\").write(\"\")\n",
    "                iFiles.append(f)\n",
    "            else:\n",
    "                print(f\"Not indexing '{f}'\\n======================\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{f} failed to index {e}\\n================\")\n",
    "            pass\n",
    "        \n",
    "    esCountIndex(index=\"test\",url=ES_URL, user=ES_USER, pw= ES_PW)\n",
    "    return iFiles\n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs(argv=sys.argv):\n",
    "    global sysargs\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(argv[0])}:\")\n",
    "    p.add_argument('-p', '--path',   type=str, required=True, help=\"path to look for files\")\n",
    "    p.add_argument('-i', '--index',  type=str, required=True, help=\"Elastic Search index\")\n",
    "    p.add_argument('-m', '--model',  type=str, required=False, default=\"all-minilm:L6-v2\", help=\"embedding model\")\n",
    "    p.add_argument('-e', '--es_url', type=str, required=False, default=ES_URL,  help=\"elastic URL\")\n",
    "    p.add_argument('-u', '--es_user',type=str, required=False, default=ES_USER, help=\"elastic user\")\n",
    "    p.add_argument('-w', '--es_pass',type=str, required=False, default=ES_PW,   help=\"elastic password\")\n",
    "    p.add_argument('-f', '--force',  required=False, default=False, action='store_true', help=\"force\")\n",
    "    p.add_argument('-j', '--just' ,  required=False, default=False, action='store_true', help=\"Just show - do not index\")\n",
    "    p.add_argument('-r', '--recurse',required=False, default=False, action='store_true', help=\"Recurse though the folder\")\n",
    "\n",
    "    sysargs=p.parse_args(argv[1:])\n",
    "    return sysargs\n",
    "\n",
    "from colabexts import utils as colabexts_utils\n",
    "if __name__ == '__main__' and not colabexts_utils.inJupyter():\n",
    "    a = addargs()\n",
    "    logger.info(f\"Indexing  {sysargs}\")\n",
    "\n",
    "    indexFromFolder(folder=a.path, force=a.force, index=a.index, url=a.es_url, recurse=a.recurse,\n",
    "                        user=a.es_user, pw= a.es_pass, model=a.model)\n",
    "\n",
    "#    indexFromFolder(sys.argv[1])\n",
    "# index, model = \"test2\", \"all-minilm:L6-v2\"\n",
    "# index, model = \"test3\", \"llama3.2:latest\"\n",
    "\n",
    "# esDeleteIndex(index)\n",
    "# esCreateIndex(index)\n",
    "\n",
    "# loadES(model, index);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 10:45:42,583 elastic_transport.transport INFO: POST http://localhost:9200/test/_count [status:200 duration:0.007s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in index 'test': {'count': 123, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'count': 123, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#esDeleteIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 11:36:19,962 gpt INFO: Indexing files from /Users/e346104/data/sageai/**: found 54 files.\n",
      "2025-05-26 11:36:19,970 elastic_transport.transport INFO: POST http://localhost:9200/test/_count [status:200 duration:0.005s]\n",
      "2025-05-26 11:36:19,975 elastic_transport.transport INFO: POST http://localhost:9200/test/_count [status:200 duration:0.003s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(path='.', index='test', model='all-minilm:L6-v2', es_url='http://localhost:9200', es_user='elastic', es_pass='elastic', force=False, just=False, recurse=False)\n",
      "Total documents in index 'test': {'count': 123, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\n",
      "Total documents in index 'test': {'count': 123, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'count': 123, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addargs('h -p. -itest'.split())\n",
    "print(sysargs)\n",
    "li=indexFromFolder(\"/Users/e346104/data/sageai\", recurse=1, just_show=0)\n",
    "esCountIndex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 11:45:33,636 elastic_transport.transport INFO: GET http://localhost:9200/ [status:200 duration:0.004s]\n",
      "2025-05-26 11:45:33,674 httpx INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-05-26 11:45:33,682 elastic_transport.transport INFO: POST http://localhost:9200/test/_search?_source_includes=metadata,text [status:200 duration:0.008s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<h3>Document, 7 : LM Digital Transformation Brief_6.28.23_PIRA_CHQ2023060067.pdf </h3> \n",
       "\n",
       "Document Name: LM Digital Transformation Brief_6.28.23_PIRA_CHQ2023060067.pdf<br><br>Compilation and integration of data across disparate tools and Standard, secure and automated approaches to collecting, organizing domains to deliver visualizations and analytical insights through\n",
       "<hr/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is the rotation axis of the solar arrays?\"\n",
    "query = \"Who is david dumdum\"\n",
    "query = \"dumdum\"\n",
    "\n",
    "index, model = \"test\", \"all-minilm:L6-v2\"\n",
    "docs = esSearchIndex(None, model=model, index_name=index, rank=1, query=query, k=5)\n",
    "print(f\"Found {len(docs)} documents\")\n",
    "format(docs[0]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 11:45:46,578 elastic_transport.transport INFO: POST http://localhost:9200/test/_search?q=%20dumdum [status:200 duration:0.007s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 documents\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the rotation axis of the solar arrays?\"\n",
    "query = \" dumdum\"\n",
    "docs = esTextSearch(index_name=index, query=query, k=5)\n",
    "print(f\"Found {len(docs)} documents\")\n",
    "if ( len(docs)):\n",
    "    format(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How to index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esclient = Elasticsearch(ES_URL, basic_auth = (ES_USER, ES_PW))\n",
    "esclient.info()\n",
    "esclient.indices.delete(index=\"test1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------\n",
    "# **** TEST ****\n",
    "#\n",
    "# Add documents to index\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def getTestDocs():\n",
    "    docs= [ \n",
    "        Document(page_content= \"Hello world!\",  metadata=dict(source= \"src\")),\n",
    "        Document(page_content= \"day11 world!\",  metadata=dict(source= \"src\")),\n",
    "        Document(page_content= \"day21 world!\",  metadata=dict(source= \"src\")),\n",
    "        Document(page_content= \"day31 world!\",  metadata=dict(source= \"src\")),\n",
    "        Document(page_content= \"day41 world!\",  metadata=dict(source= \"src\")),\n",
    "    ]\n",
    "    return docs\n",
    "\n",
    "# Index documents \n",
    "def test1(docs=None):\n",
    "    if (docs is None):\n",
    "        docs = getTestDocs()\n",
    "        \n",
    "    embed= getEmbedding()\n",
    "\n",
    "    # STEP 1. lets delete the test index \n",
    "    print(\"Deleteing test index\")\n",
    "    esDeleteIndex(\"test\")\n",
    "    esCreateIndex(\"test\")\n",
    "\n",
    "    es = dict(es_url= ES_URL, es_user=ES_USER, es_password=ES_PW)\n",
    "\n",
    "    print(\"Add to elastic\")\n",
    "    v = add_to_es(docs, es, index=\"test\", embed=embed)\n",
    "    return v\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# Retrieve documents\n",
    "# \n",
    "def test2(v, q=\"hello\"):\n",
    "    print(\"==> TEST Vector Retriever ==>\")\n",
    "    ret = esVectorSearch( v, q, 3)\n",
    "    for r in ret:   \n",
    "        print(f\"===> {str(r)[0:64]}\")\n",
    "\n",
    "    print(\"==> RERANK Vector Retriever ==>\")\n",
    "    ret = rerank( q, ret)\n",
    "    for r in ret:   \n",
    "        print(f\"===> {str(r)[0:64]}\")\n",
    "\n",
    "    print(\"==> TEST keyword Retriever ==>\")\n",
    "    ret = esTextSearch(  q, 3)\n",
    "    for r in ret:   \n",
    "        print(f\"===> {str(r)[0:64]}\")\n",
    "\n",
    "    print(\"==> RERANK KW Retriever ==>\")\n",
    "    #ret = rerank( q, ret)\n",
    "    #for r in ret:   \n",
    "    #    print(r)\n",
    "\n",
    "    return ret\n",
    "\n",
    "from . import extract_text\n",
    "def test():\n",
    "    docs = extract_text.getChunks(\"~/Desktop/data/LLM/sample.pdf\")\n",
    "    v = test1(docs)\n",
    "    ret = test2(v,\"day11\")\n",
    "    return v\n",
    "\n",
    "v = test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"Arianespace has processing and launch\"\n",
    "ret = esVectorSearch( v, q, 3)\n",
    "print(\"***==>  Vector Retriever ==>\")\n",
    "for r in ret:   \n",
    "    print(f\"===> {str(r)[0:64]}\")\n",
    "    #print(r)\n",
    "\n",
    "'''print(\"\\n\\n***==> RERANK Vector Retriever ==>\")\n",
    "ret = rerank( q, ret)\n",
    "for r in ret:   \n",
    "    print(f\"===> {str(r)[0:64]}\")\n",
    "\n",
    "print(\"\\n\\n***==> TEST keyword Retriever ==>\")\n",
    "ret = esTextSearch(  q, 3)\n",
    "for r in ret:   \n",
    "    print(f\"===> {str(r)[0:64]}\")\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
