{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuzzPHd6gIUQ",
    "outputId": "a34fedfd-7340-43a9-90d7-4952f665db1e"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "\n",
    "# See docs/elastic.md to start Elastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic DB Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 08:21:41,332 app.mangorest INFO: ************* MANGO REST API 2.1****************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******\n",
      "***** ERROR LOADING\n",
      "No module named 'geoapp'\n",
      "*******\n",
      "*******\n",
      "Initializing web services: /Users/e346104/git/DS/apps/gpt/notebooks\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'services'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melasticsearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Elasticsearch\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmangorest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmango\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webapi\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf_parser_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pdf_parser\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataframe_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge_records,metadata_chunks,chunk_dict_to_list, chunks_to_doc_obj\n\u001b[1;32m     23\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "File \u001b[0;32m~/git/DS/apps/gpt/notebooks/gpt/__init__.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt/index_docs.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m index_docs\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#from . import chatgpt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt/whispermod.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "File \u001b[0;32m~/git/DS/apps/gpt/notebooks/gpt/index_docs.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#------------------------------------------INITIALIZE the DB-------------------- \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/utils/geo_utils/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mpath: sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/utils/geo_utils/\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmyjson\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m myjson\n\u001b[1;32m     35\u001b[0m MYDB \u001b[38;5;241m=\u001b[39m myjson(base\u001b[38;5;241m=\u001b[39mBASE, db\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINDICES\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m INDEX_TABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindexes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'services'"
     ]
    }
   ],
   "source": [
    "#%%writefile ../db_elastic.py\n",
    "#!/usr/bin/env python \n",
    "\n",
    "import os, sys, logging, argparse, glob\n",
    "#sys.path.append(os.path.expanduser(\"~/.django\") )\n",
    "#sys.path.append(os.path.expanduser(\"gpt\") )\n",
    "\n",
    "from importlib import metadata\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_elasticsearch import (\n",
    "    BM25Strategy,\n",
    "    DenseVectorStrategy,\n",
    "    ElasticsearchStore,\n",
    ")\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from mangorest.mango import webapi\n",
    "from gpt.pdf_parser_tools import pdf_parser\n",
    "from dataframe_tools import merge_records,metadata_chunks,chunk_dict_to_list, chunks_to_doc_obj\n",
    "\n",
    "logger = logging.getLogger( \"gpt\" )\n",
    "\n",
    "# You can set these in your ~/.django/my_config\n",
    "\n",
    "ES_URL, ES_USER, ES_PW  = \"http://localhost:9200\", \"elastic\", \"elastic\"\n",
    "ES_CNX= dict(es_url= ES_URL, es_user= ES_USER, es_password=ES_PW)\n",
    "\n",
    "if (os.path.exists(os.path.expanduser(\"~/.django/my_config.py\"))):\n",
    "    import my_config\n",
    "    try:\n",
    "        from my_config import ES_URL, ES_USER, ES_PW\n",
    "        ES_CNX= dict(es_url= ES_URL, es_user= ES_USER, es_password=ES_PW)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "_ES_STARTEGIES = {\n",
    "    \"hnsw\":     DenseVectorStrategy(), \n",
    "    \"bm25\":     BM25Strategy(),\n",
    "    \"hybrid\":   DenseVectorStrategy(hybrid=True, rrf=False),\n",
    "    \"sparse\":   None,\n",
    "    \"exact\":    None,\n",
    "}\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def esDeleteIndex(index):\n",
    "    esclient = Elasticsearch(ES_URL, basic_auth = (ES_USER, ES_PW))\n",
    "    esclient.info()\n",
    "    try:\n",
    "        esclient.indices.delete(index=index)\n",
    "    except:\n",
    "        pass\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def esCreateIndex(index):\n",
    "    esclient = Elasticsearch(ES_URL, basic_auth = (ES_USER, ES_PW))\n",
    "    esclient.indices.create(index=index)\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def getEmbedding(model=\"all-minilm:L6-v2\", base_url = \"http://127.0.0.1:11434/\"):\n",
    "    e = OllamaEmbeddings( model = model, base_url =base_url )\n",
    "    return e\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def add_to_es( docs: list[Document], es_cnx: dict, index: str, embed, strategy= \"hnsw\" ):\n",
    "    strat = _ES_STARTEGIES[strategy]\n",
    "    vectorstore = None\n",
    "    for i in range(0, len(docs), 20000):\n",
    "        vectorstore = ElasticsearchStore.from_documents(\n",
    "            documents=docs[i : min(i + 20000, len(docs))],\n",
    "            embedding=embed,\n",
    "            **es_cnx,\n",
    "            index_name=index,\n",
    "            bulk_kwargs={\n",
    "                \"chunk_size\": 100,\n",
    "            },\n",
    "            strategy=strat,\n",
    "        )\n",
    "    return vectorstore\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def es_retriever( es_cnx: dict, index: str, embed, strategy=\"hnsw\", k= 10 ):\n",
    "    strat = _ES_STARTEGIES[strategy]\n",
    "\n",
    "    v = ElasticsearchStore( **es_cnx, embedding=embed, index_name=index, strategy=strat)\n",
    "    return v.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "def esVectorSearch( retreiver, q, k=10):\n",
    "        ret = retreiver.as_retriever(search_kwargs={\"k\": k}).invoke(q)\n",
    "        \n",
    "        h = {r.page_content:r for r in ret}\n",
    "        if len(h) != len(ret):\n",
    "            ret = [v for v in h.values()]\n",
    "            \n",
    "        return ret\n",
    "\n",
    "@webapi(\"/gpt/esSearchIndex/\")\n",
    "def esSearchIndex(request, index_name, query, model=\"llama3.2\", user=\"\", es_url=\"\", \n",
    "                    es_user=\"\", es_pass=\"\", k=10, rank=1, **kwargs):\n",
    "\n",
    "    #print(f\"\\n{locals()}\\n\")\n",
    "        \n",
    "    if (not es_url):\n",
    "        es = dict(es_url= ES_URL, es_user=ES_USER, es_password=ES_PW)\n",
    "    else:\n",
    "        es = dict(es_url= es_url, es_user=es_user, es_password=es_pass)\n",
    "\n",
    "    #model = \"llama3.2\" #lets force the embedding for now\n",
    "    embed = getEmbedding(model=model) \n",
    "\n",
    "    \n",
    "    if ( rank):\n",
    "        v = es_retriever(es, index=index_name, embed=embed, k=k*2)\n",
    "        docs = v.invoke(query)\n",
    "        if (len(docs)):\n",
    "            ranked = rerank( query, docs)\n",
    "            docs = [Document(page_content=r['text'], metadata=r['metadata']) for r in ranked[0:k]]\n",
    "    else:\n",
    "        v = es_retriever(es, index=index_name, embed=embed, k=k)\n",
    "        docs = v.invoke(query)\n",
    "\n",
    "    h = {r.page_content: r for r in docs}\n",
    "    if len(h) != len(docs):\n",
    "        docs = [v for v in h.values()]\n",
    "    \n",
    "    ret = []\n",
    "    for d in docs:\n",
    "        ret.append(dict(page_content=d.page_content, metadata=d.metadata))\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def esTextSearch(q, k=10, index=\"test\", url = ES_URL, user=ES_USER, pw= ES_PW):\n",
    "    esclient = Elasticsearch(url, basic_auth = (user, pw))\n",
    "    res = esclient.search(index=index,  q=q, size=k)\n",
    "\n",
    "    ret = []\n",
    "    for i,r in enumerate(res['hits']['hits']):\n",
    "        pc = r['_source']['text']\n",
    "        mt = r['_source']['metadata']\n",
    "        ret.append(Document(page_content = pc, metadata=mt))\n",
    "        #print(i, \" ==>\", )\n",
    "    return ret\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def rerank(q, ret):\n",
    "    from flashrank import (Ranker, RerankRequest,)\n",
    "\n",
    "    ranker = Ranker(\"ms-marco-MiniLM-L-12-v2\", os.path.expanduser(\"~/.cache/RERANKER/\"))\n",
    "    rerankrequest = RerankRequest(\n",
    "        query=q, passages=[{\"text\": d.page_content, \"metadata\": d.metadata} for d in ret]\n",
    "    )\n",
    "    reranked = ranker.rerank(rerankrequest)\n",
    "    return reranked\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def getchunksFromPDF(filename):\n",
    "    filename = os.path.expanduser(filename)\n",
    "    \n",
    "    record = pdf_parser(filename)\n",
    "    # Seems like there is not data excepts figures and tables in the pdf\n",
    "    if ( not record ):\n",
    "        logger.info(\"Hmmmm not records found in PDF file!\")\n",
    "        return [] \n",
    "\n",
    "    merged = merge_records(record)\n",
    "\n",
    "    docName = os.path.basename(filename)\n",
    "    chunk_dict = metadata_chunks(merged,docName)\n",
    "    chunks = chunk_dict_to_list(chunk_dict)\n",
    "    docs = chunks_to_doc_obj(chunks, docName )\n",
    "    return docs\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def getchunksFromTxt(filename, chunk_overlap_ratio=0.2, chunk_size=8000):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap= int(chunk_overlap_ratio * chunk_size),\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    texts = text_splitter.split_text(text)\n",
    "    chunks = []\n",
    "    for t in texts:\n",
    "        chunks.append(Document( page_content=t,  metadata={\"source\": filename}))\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# This is standing by itself - should be called by indexFromFolder\n",
    "# can be multi tasked \n",
    "def loadES( model=\"llama3.2\", index=\"\", filename = '~/data/gpt/test-files/HS4_SGS1_V1S7.pdf',\n",
    "           es_url=ES_URL , es_user=ES_USER, es_password=ES_PW ):\n",
    "    \n",
    "    docs = []\n",
    "    if ( filename.endswith(\".pdf\")):\n",
    "        docs = getchunksFromPDF(filename)\n",
    "    elif( filename.endswith(\".txt\")):\n",
    "        docs = getchunksFromTxt(filename)\n",
    "    else:\n",
    "        logger.info(\"Indexing only PDF files now!! :)\")\n",
    "\n",
    "    if (not docs):\n",
    "        return docs\n",
    "    embed= getEmbedding(model)\n",
    "    es = dict(es_url=es_url , es_user=es_user, es_password=es_password)\n",
    "    v = add_to_es(docs, es, index=index, embed=embed)\n",
    "\n",
    "    return docs\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def indexFromFolderOLD(folder=\"\", force=0, index=\"test\", url=ES_URL, user=ES_USER, pw= ES_PW, model=\"llama3.2\"):\n",
    "    import extract_text\n",
    "\n",
    "    folder = os.path.expanduser(folder) + \"/*\"\n",
    "    files = [f for f in glob.glob(folder, recursive=0) if os.path.isfile(f)]\n",
    "\n",
    "    embed= getEmbedding(model)\n",
    "    es = dict(es_url=url , es_user=user, es_password=pw)\n",
    "\n",
    "    for f in files:\n",
    "        marker = f\".{f}.{index}.indexed\"\n",
    "        if f.endswith(\".indexed\") or (os.path.exists( marker) and not force):\n",
    "            continue;\n",
    "\n",
    "        logger.info(f\"Indexing {f}\")        \n",
    "        try:\n",
    "            docs = extract_text.getChunks(f)\n",
    "            v = add_to_es(docs, es, index=index, embed=embed)\n",
    "            open(marker, \"w\").write(\"\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{f} failed to index\")\n",
    "            pass\n",
    "\n",
    "    return files\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def indexFromFolder(folder=\"\", force=0, index=\"test\", url=ES_URL, user=ES_USER, pw= ES_PW, model=\"llama3.2\"):\n",
    "    folder = os.path.expanduser(folder) + \"/*\"\n",
    "    files = [f for f in glob.glob(folder, recursive=0) if os.path.isfile(f)]\n",
    "\n",
    "    embed= getEmbedding(model)\n",
    "    es = dict(es_url=url , es_user=user, es_password=pw)\n",
    "\n",
    "    logger.info(f\"Indexing files from {folder}: found {len(files)} files.\")        \n",
    "\n",
    "    iFiles = []\n",
    "    for f in files:\n",
    "        bn = os.path.basename(f)\n",
    "        dn = os.path.dirname(f)\n",
    "        marker = f\"{dn}/.{bn}.{index}.indexed\"\n",
    "\n",
    "        if f.endswith(\".indexed\") or (os.path.exists( marker) and not force):\n",
    "            continue;\n",
    "\n",
    "        logger.info(f\"Indexing {f}\")        \n",
    "        try:\n",
    "            loadES(model, index, f, url, user, pw)\n",
    "            open(marker, \"w\").write(\"\")\n",
    "            iFiles.append(f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{f} failed to index {e}\")\n",
    "            pass\n",
    "    return iFiles\n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs(argv=sys.argv):\n",
    "    global sysargs\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(argv[0])}:\")\n",
    "    p.add_argument('-p', '--path',   type=str, required=True, help=\"where files are located to index\")\n",
    "    p.add_argument('-i', '--index',  type=str, required=True, help=\"Elastic Search index\")\n",
    "    p.add_argument('-m', '--model',  type=str, required=False, default=\"all-minilm:L6-v2\", help=\"embedding model\")\n",
    "    p.add_argument('-e', '--es_url', type=str, required=False, default=ES_URL,  help=\"elastic URL\")\n",
    "    p.add_argument('-u', '--es_user',type=str, required=False, default=ES_USER, help=\"elastic user\")\n",
    "    p.add_argument('-w', '--es_pass',type=str, required=False, default=ES_PW,   help=\"elastic password\")\n",
    "    p.add_argument('-f', '--force',  required=False, default=False, action='store_true', help=\"force\")\n",
    "\n",
    "    sysargs=p.parse_args(argv[1:])\n",
    "    return sysargs\n",
    "\n",
    "from colabexts import utils as colabexts_utils\n",
    "if __name__ == '__main__' and not colabexts_utils.inJupyter():\n",
    "    a = addargs()\n",
    "    logger.info(f\"Indexing  {sysargs}\")\n",
    "\n",
    "    indexFromFolder(folder=a.path, force=a.force, index=a.index, url=a.es_url, \n",
    "                        user=a.es_user, pw= a.es_pass, model=a.model)\n",
    "\n",
    "#    indexFromFolder(sys.argv[1])\n",
    "# index, model = \"test2\", \"all-minilm:L6-v2\"\n",
    "# index, model = \"test3\", \"llama3.2:latest\"\n",
    "\n",
    "# esDeleteIndex(index)\n",
    "# esCreateIndex(index)\n",
    "\n",
    "# loadES(model, index);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, model = \"test2\", \"all-minilm:L6-v2\"\n",
    "index, model = \"test1\", \"llama3.2:latest\"\n",
    "\n",
    "#esDeleteIndex(index)\n",
    "#esCreateIndex(index)\n",
    "\n",
    "loadES(model, index, filename = '~/data/gpt/test-files/HS4_SGS1_V1S7.pdf');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------\n",
    "def getchunks(filename):\n",
    "    filename = os.path.expanduser(filename)\n",
    "    \n",
    "    record = pdf_parser(filename)\n",
    "    merged = merge_records(record)\n",
    "\n",
    "    docName = os.path.basename(filename)\n",
    "    chunk_dict = metadata_chunks(merged,docName)\n",
    "    chunks = chunk_dict_to_list(chunk_dict)\n",
    "    docs = chunks_to_doc_obj(chunks, docName )\n",
    "    return docs\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# This is standing by itself - should be called by indexFromFolder\n",
    "# can be multi tasked \n",
    "def loadES( model=\"llama3.2\", index=\"\", filename = '~/data/gpt/test-files/HS4_SGS1_V1S7.pdf',\n",
    "           es_url=ES_URL , es_user=ES_USER, es_password=ES_PW ):\n",
    "    \n",
    "    docs = getchunks(filename)\n",
    "    embed= getEmbedding(model)\n",
    "    es = dict(es_url=es_url , es_user=es_user, es_password=es_password)\n",
    "    v = add_to_es(docs, es, index=index, embed=embed)\n",
    "\n",
    "    return docs\n",
    "\n",
    "filename=\"/Users/e346104/data/gpt/test-files/HS4SGS1v3s3_table.pdf\"\n",
    "#docs = getchunks(filename)\n",
    "record = pdf_parser(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the rotation axis of the solar arrays?\"\n",
    "index, model = \"test2\", \"all-minilm:L6-v2\"\n",
    "index, model = \"test3\", \"llama3.2\"\n",
    "#model =\"llama3.2\"\n",
    "esSearchIndex(None, model=model, index_name=index, rank=1, query=query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esclient = Elasticsearch(ES_URL, basic_auth = (ES_USER, ES_PW))\n",
    "esclient.info()\n",
    "esclient.indices.delete(index=\"test1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------\n",
    "# **** TEST ****\n",
    "#\n",
    "# Add documents to index\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def getTestDocs():\n",
    "    docs= [ \n",
    "        Document(page_content= \"Hello world!\",  metadata=dict(source= \"src\")),\n",
    "        Document(page_content= \"day11 world!\",  metadata=dict(source= \"src\")),\n",
    "        Document(page_content= \"day21 world!\",  metadata=dict(source= \"src\")),\n",
    "        Document(page_content= \"day31 world!\",  metadata=dict(source= \"src\")),\n",
    "        Document(page_content= \"day41 world!\",  metadata=dict(source= \"src\")),\n",
    "    ]\n",
    "    return docs\n",
    "\n",
    "# Index documents \n",
    "def test1(docs=None):\n",
    "    if (docs is None):\n",
    "        docs = getTestDocs()\n",
    "        \n",
    "    embed= getEmbedding()\n",
    "\n",
    "    # STEP 1. lets delete the test index \n",
    "    print(\"Deleteing test index\")\n",
    "    esDeleteIndex(\"test\")\n",
    "    esCreateIndex(\"test\")\n",
    "\n",
    "    es = dict(es_url= ES_URL, es_user=ES_USER, es_password=ES_PW)\n",
    "\n",
    "    print(\"Add to elastic\")\n",
    "    v = add_to_es(docs, es, index=\"test\", embed=embed)\n",
    "    return v\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# Retrieve documents\n",
    "# \n",
    "def test2(v, q=\"hello\"):\n",
    "    print(\"==> TEST Vector Retriever ==>\")\n",
    "    ret = esVectorSearch( v, q, 3)\n",
    "    for r in ret:   \n",
    "        print(f\"===> {str(r)[0:64]}\")\n",
    "\n",
    "    print(\"==> RERANK Vector Retriever ==>\")\n",
    "    ret = rerank( q, ret)\n",
    "    for r in ret:   \n",
    "        print(f\"===> {str(r)[0:64]}\")\n",
    "\n",
    "    print(\"==> TEST keyword Retriever ==>\")\n",
    "    ret = esTextSearch(  q, 3)\n",
    "    for r in ret:   \n",
    "        print(f\"===> {str(r)[0:64]}\")\n",
    "\n",
    "    print(\"==> RERANK KW Retriever ==>\")\n",
    "    #ret = rerank( q, ret)\n",
    "    #for r in ret:   \n",
    "    #    print(r)\n",
    "\n",
    "    return ret\n",
    "\n",
    "from . import extract_text\n",
    "def test():\n",
    "    docs = extract_text.getChunks(\"~/Desktop/data/LLM/sample.pdf\")\n",
    "    v = test1(docs)\n",
    "    ret = test2(v,\"day11\")\n",
    "    return v\n",
    "\n",
    "v = test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"Arianespace has processing and launch\"\n",
    "ret = esVectorSearch( v, q, 3)\n",
    "print(\"***==>  Vector Retriever ==>\")\n",
    "for r in ret:   \n",
    "    print(f\"===> {str(r)[0:64]}\")\n",
    "    #print(r)\n",
    "\n",
    "'''print(\"\\n\\n***==> RERANK Vector Retriever ==>\")\n",
    "ret = rerank( q, ret)\n",
    "for r in ret:   \n",
    "    print(f\"===> {str(r)[0:64]}\")\n",
    "\n",
    "print(\"\\n\\n***==> TEST keyword Retriever ==>\")\n",
    "ret = esTextSearch(  q, 3)\n",
    "for r in ret:   \n",
    "    print(f\"===> {str(r)[0:64]}\")\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
