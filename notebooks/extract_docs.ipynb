{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting genai_utils/extract_docs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile genai_utils/extract_docs.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys, os, logging,datetime, docx, pandas as pd\n",
    "logger = logging.getLogger( \"genai_utils\" )\n",
    "from langchain_core.documents import Document\n",
    "from genai_utils import pdf_parser\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Following functions extract chunks\n",
    "# ------------------------------------------------------------------------------------------\n",
    "def extractDocx(file):\n",
    "    document = docx.Document(file)\n",
    "\n",
    "    # STEP 1: extract tables \n",
    "    tables=[]\n",
    "    for table in document.tables:\n",
    "        data = []\n",
    "        keys = None\n",
    "        for i, row in enumerate(table.rows):\n",
    "            text = (cell.text for cell in row.cells)\n",
    "\n",
    "            if i == 0:\n",
    "                keys = tuple(text)\n",
    "                continue\n",
    "            row_data = dict(zip(keys, text))\n",
    "            data.append(row_data)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        t = tables.append(Document(metadata={\"head\": \"table\", 'source': file}, \n",
    "                                       page_content= str(df)))\n",
    "        tables.append(t)\n",
    "\n",
    "    # STEP 2: extract text \n",
    "    paras = []\n",
    "    paraTexts = []\n",
    "    secHeader = \"\"\n",
    "\n",
    "    for para in document.paragraphs:\n",
    "        if (not para.text.strip()):\n",
    "            continue;\n",
    "        \n",
    "        if para.style.name != \"Normal\" and \"Paragraph\" not in para.style.name:\n",
    "            #print(\"==>\", para.style.name)\n",
    "            if ( len(paraTexts) > 0):\n",
    "                paras.append(Document(metadata={\"head\": secHeader, 'source': file}, \n",
    "                                       page_content=\"\\n\".join(paraTexts)))\n",
    "                paraTexts = []\n",
    "\n",
    "            secHeader = para.text \n",
    "            continue;\n",
    "        \n",
    "        paraTexts.append(para.text)\n",
    "\n",
    "    docs =  tables + paras\n",
    "    return docs\n",
    "# ---------------------------------------------------------------------------------------\n",
    "'''\n",
    "OLD FUNCTION\n",
    "\n",
    "def getChunks(file, chunk_size=8000, overlap=256 ):\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    txt= extractText( file= file)\n",
    "\n",
    "    split = RecursiveCharacterTextSplitter(\n",
    "        chunk_size= chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    docs = []\n",
    "    for txt in split.split_text(txt):\n",
    "        d = Document(page_content= txt,  metadata=dict(source= file))\n",
    "        docs.append(d)\n",
    "\n",
    "    return docs\n",
    "'''\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def getchunksFromTxt(filename, chunk_overlap_ratio=0.2, chunk_size=8000):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap= int(chunk_overlap_ratio * chunk_size),\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    texts = text_splitter.split_text(text)\n",
    "    docs = []\n",
    "    for t in texts:\n",
    "        docs.append(Document( page_content=t,  metadata={\"source\": filename}))\n",
    "    return docs\n",
    "#-----------------------------------------------------------------------------------------    \n",
    "def extractDocs(request=None, file=None, **kwargs):\n",
    "    ret = f\"Unknown file type {file}\"\n",
    "\n",
    "    if ( request and not file):\n",
    "        for f in request.FILES.getlist('file'):\n",
    "            content = f.read()\n",
    "            #fileIO = io.BytesIO(content)\n",
    "            file = f\"/tmp/{str(f)}\"\n",
    "            with open(file, \"wb\") as f:\n",
    "                f.write(content)\n",
    "\n",
    "\n",
    "    if (file.endswith(\"doc\") or file.endswith(\"docx\") ):\n",
    "        ret =  extractDocx(file)\n",
    "    elif (file.endswith(\"txt\") or file.endswith(\"md\") ):\n",
    "        ret =  getchunksFromTxt(file)\n",
    "    elif (file.endswith(\"pdf\") ):\n",
    "        ret = pdf_parser.getDocsFromPDF(file)\n",
    "    elif (file.endswith(\"xlsx\") or file.endswith(\"xls\")):\n",
    "        df = pd.read_excel(file)\n",
    "        ret = df.to_html()\n",
    "        d = Document(page_content= ret,  metadata=dict(source= file))\n",
    "        ret=[d]\n",
    "    elif file.endswith(\"csv\") :\n",
    "        df = pd.read_csv(file)\n",
    "        ret = df.to_html()\n",
    "        d = Document(page_content= ret,  metadata=dict(source= file))\n",
    "        ret=[d]\n",
    "    elif file:\n",
    "        with open(file, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "            ret = f.read()\n",
    "        d = Document(page_content= ret,  metadata=dict(source= file))\n",
    "        ret=[d]\n",
    "    else:\n",
    "        ret = []\n",
    "    \n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 \n",
      "\n",
      " page_content='Document Name: sample.pdf\n",
      "\n",
      "This Userâ€™s Manual provides essential data on the Ariane 5 launch System, which together with the Soyuz and Vega launch vehicles, constitutes the European space transportation union. These three launch systems are operated by Arianespace from the Guiana Space Centre (CSG). This document contains the essential data which is necessary: to assess compatibility of a spacecraft and spacecraft mission with launch system, to constitute the general launch service provisions and specifications, and to initiate the preparation of all technical and operational documentation related to a launchof any' metadata={'source': 'sample.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "file=\"/Users/e346104/Desktop/data/LLM/sample.pdf\"\n",
    "#file=\"/Users/e346104/Desktop/data/LLM/sample.docx\"\n",
    "#file=\"/Users/e346104/Desktop/data/LLM/sample.txt\"\n",
    "\n",
    "txts = extractDocs(file=file)\n",
    "print(len(txts), \"\\n\\n\", txts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
