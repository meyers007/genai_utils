{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3af86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting genai_utils/index_images_es.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"genai_utils/index_images_es.py\"\n",
    "#!/usr/bin/env python \n",
    "\n",
    "import pdfplumber, os, base64, glob, tqdm, json\n",
    "from io import BytesIO\n",
    "import io, base64\n",
    "from PIL import Image\n",
    "from IPython.display import HTML\n",
    "from genai_utils.describe_image import describe_image\n",
    "from genai_utils import db_elastic\n",
    "\n",
    "def _extractImagesFromPDF(file=None, **kwargs):\n",
    "    assert file.endswith(\"pdf\"), \"Called with non PDF File!!\"\n",
    "\n",
    "    ret = {}\n",
    "    txt = []\n",
    "    with pdfplumber.open(file) as doc:\n",
    "        for pageNumber, page in enumerate(doc.pages):\n",
    "            images = page.images\n",
    "            txt.append(page.extract_text())\n",
    "            for image_index, img in enumerate(images):\n",
    "                bbox = (img['x0'], img['top'], img['x1'], img['bottom'])\n",
    "                image = page.within_bbox(bbox).to_image()\n",
    "                pil_image = image.original\n",
    "                imageRGB = pil_image.convert(\"RGB\")\n",
    "                b = BytesIO()\n",
    "                imageRGB.save(b, format='PNG')\n",
    "                b.seek(0)\n",
    "                br= b.read()\n",
    "                b64Image = base64.b64encode(br).decode(\"utf-8\")\n",
    "                url = \"data:image/jpg;base64, \" + b64Image\n",
    "                #img = f\"<img src='{url}' >\"\n",
    "                #display (HTML(img))\n",
    "                ret[url] = 1\n",
    "    ret = [r for r in ret.keys()]\n",
    "    return dict(images=ret, texts=txt)\n",
    "\n",
    "def indexImagesFromPDF(file, savedir=\"/tmp/genai_utils/\", verbose =0):\n",
    "    ret = _extractImagesFromPDF(file)\n",
    "        \n",
    "    if ( savedir is None or not savedir):\n",
    "        return ret, None\n",
    "    files = []\n",
    "    for i, img in enumerate(ret['images']):\n",
    "        img1=img[img.index(\",\")+1:].strip()\n",
    "        imgd = Image.open(io.BytesIO(base64.decodebytes(img1.encode()) ))\n",
    "        \n",
    "        bname = os.path.basename(file)\n",
    "        sfile = f\"{savedir}/{bname}__{i}.png\"\n",
    "        #os.makedirs(savedir, exist_ok=True)\n",
    "        imgd.save(sfile)\n",
    "        files.append(sfile)\n",
    "        print(f\"Saved {sfile}\")\n",
    "        if ( verbose):\n",
    "            display(HTML(f\"<img src='{img}'> \"))\n",
    "            print(ret['texts'][i][0:128])\n",
    "    return ret, files\n",
    "    \n",
    "def index_directory(directory, outf= {}, recurse=0):\n",
    "    pngs = glob.glob(os.path.join(directory, '**/*.png') , recursive=recurse)\n",
    "    jpgs = glob.glob(os.path.join(directory, '**/*.jpg') , recursive=recurse)\n",
    "    jpes = glob.glob(os.path.join(directory, '**/*.jpeg'), recursive=recurse)\n",
    "    pdfs = glob.glob(os.path.join(directory, '**/*.pdf') , recursive=recurse)\n",
    "\n",
    "    images= []\n",
    "    for pdfFile in tqdm.tqdm(pdfs):\n",
    "        print(f\"Getting images from {pdfFile}\")\n",
    "        ret, files = indexImagesFromPDF(pdfFile)\n",
    "        images.extend(files)\n",
    "    \n",
    "    image_paths = [*pngs, *jpgs, *jpes, *images]\n",
    "    for image_path in tqdm.tqdm(image_paths):\n",
    "        if image_path in outf:\n",
    "            continue\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_data = f.read()\n",
    "        try:\n",
    "            description = \"\"\n",
    "            description = describe_image(image_data)\n",
    "            print(f\"Indexed {image_path}: {description}\")\n",
    "            outf[image_path] = description\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to index {image_path}: {e}\")\n",
    "            pass\n",
    "        \n",
    "    return outf\n",
    "\n",
    "def getDocs(outf):\n",
    "    from langchain_core.documents import Document\n",
    "\n",
    "    docs = []\n",
    "    for k,v in outf.items():\n",
    "        print(k, v[0:32])\n",
    "        d = Document(page_content=v, metadata=dict(source=k) )\n",
    "        docs.append(d)\n",
    "    return docs\n",
    "\n",
    "def save(outf, file=\"/tmp/genai_utils/images_dir.json\"):\n",
    "    with open(file, \"wt\") as f:\n",
    "        f.write(json.dumps(outf))\n",
    "    \n",
    "def load(file=\"/tmp/genai_utils/images_dir.json\"):\n",
    "    outf = {}\n",
    "    if ( os.path.exists(file)):\n",
    "        with open(file, \"rt\") as f:\n",
    "            outf = json.loads( f.read() )\n",
    "    return outf\n",
    "\n",
    "outf=load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3053df94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 384798.53it/s]\n",
      "2025-05-27 18:27:24,103 elastic_transport.transport INFO: GET http://localhost:9200/ [status:200 duration:0.005s]\n",
      "2025-05-27 18:27:24,106 elastic_transport.transport INFO: HEAD http://localhost:9200/sageai_images [status:200 duration:0.002s]\n",
      "2025-05-27 18:27:24,216 httpx INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/a.png The image is a flowchart depicti\n",
      "/tmp/gg/a.png The image is a flowchart depicti\n",
      "/tmp/genai_utils/z4.pdf__0.png The image is a flowchart depicti\n",
      "/tmp/genai_utils/z4.pdf__1.png The image is a line graph with a\n",
      "/tmp/genai_utils/z4.pdf__3.png The image is a bar chart with a \n",
      "/tmp/genai_utils/z4.pdf__2.png The image is a line graph with a\n",
      "/tmp/genai_utils/z4.pdf__4.png The image is a line graph with a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:27:24,256 elastic_transport.transport INFO: PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.038s]\n",
      "2025-05-27 18:27:24,265 elastic_transport.transport INFO: POST http://localhost:9200/sageai_images/_count [status:200 duration:0.004s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in index 'sageai_images': {'count': 6, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'count': 6, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from genai_utils import db_elastic\n",
    "\n",
    "outf = index_directory(\"/tmp/**\", outf=outf, recurse=1)\n",
    "\n",
    "m, url, user,pw = \"all-minilm:L6-v2\", \"http://localhost:9200\", \"elastic\", \"elastic\"\n",
    "index = \"sageai_images\"\n",
    "\n",
    "db_elastic.loadES( model=m, index=index, es_url=url , es_user=user, es_pass=pw, docs=getDocs(outf) )\n",
    "db_elastic.esCountIndex(index=index, es_url=url, es_user=user, es_pass= pw)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f78883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:28:17,449 elastic_transport.transport INFO: GET http://localhost:9200/ [status:200 duration:0.005s]\n",
      "2025-05-27 18:28:17,490 httpx INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-05-27 18:28:17,502 elastic_transport.transport INFO: POST http://localhost:9200/sageai_images/_search?_source_includes=metadata,text [status:200 duration:0.011s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'page_content': 'The image is a flowchart depicting a system architecture for processing and summarizing audio/video meetings. The flowchart is divided into two main sections: the user interface and the processing pipeline. \\n\\nIn the user interface section, labeled \"User Interface,\" there is a box labeled \"Upload Meeting (audio/video)\" which represents the input of a meeting recording. This input is directed to a box labeled \"Diarize and Transcript,\" which is part of the processing pipeline. The diarization and transcription process is facilitated by AWS Transcribe, as indicated by the text \"AWS Transcribe\" within the box.\\n\\nThe output from the \"Diarize and Transcript\" box is labeled \"Transcript\" and is directed to a box labeled \"BART Summarizer.\" This box represents the use of the BART model for summarizing the transcribed content.\\n\\nThe flowchart also includes a section labeled \"LLM\" which stands for Large Language Model. This section includes a box labeled \"GPT-3 Turbo\" and another labeled \"LangChain.\" These components are part of the processing pipeline and are connected to a \"Vector Database\" and \"Storage\" section. The \"Vector Database\" and \"Storage\" sections are represented by stacked boxes, indicating a storage or database structure.\\n\\nThe \"LLM\" section is connected to the \"Transcript\" box via a dotted line, suggesting a potential interaction or data flow between the LLM and the transcript. The \"LLM\" section is also connected to a \"Summary Clustering\" box, which is part of the output section of the flowchart.\\n\\nThe overall flowchart illustrates the process of uploading a meeting, diarizing and transcribing it, summarizing the transcript using the BART model, and potentially clustering the summary using an LLM. The system appears to be designed for summarizing and organizing large volumes of audio/video meeting content.',\n",
       " 'metadata': {'source': '/tmp/genai_utils/z4.pdf__0.png'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = db_elastic.esSearchIndex(None, index= index, query=\"SAda\")\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb55ecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:29:06,895 elastic_transport.transport INFO: POST http://localhost:9200/sageai_images/_search?q=Sada [status:200 duration:0.015s]\n"
     ]
    }
   ],
   "source": [
    "res = db_elastic.esTextSearch(query=\"Sada\", index= index, )\n",
    "if (len(res) > 0):\n",
    "    res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bae49",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95167aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file= os.path.expanduser(\"~/Desktop/data/z4.pdf\")\n",
    "ret = indexImagesFromPDF(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e908cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
